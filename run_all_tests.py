#!/usr/bin/env python3
"""
StrategyBuilder Pro - Comprehensive Test Suite Runner
Runs all tests and generates detailed report for beta release validation.
"""

import sys
import os
import unittest
import time
from io import StringIO
from datetime import datetime
from typing import Dict, List, Tuple
import json

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))


class ColoredTextTestResult(unittest.TextTestResult):
    """Enhanced test result with colored output and detailed tracking."""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_results = []
        self.module_stats = {}

    def startTest(self, test):
        super().startTest(test)
        self.test_start_time = time.time()

    def addSuccess(self, test):
        super().addSuccess(test)
        elapsed = time.time() - self.test_start_time
        self.test_results.append({
            'test': str(test),
            'status': 'PASS',
            'time': elapsed,
            'module': test.__class__.__module__
        })

    def addError(self, test, err):
        super().addError(test, err)
        elapsed = time.time() - self.test_start_time
        self.test_results.append({
            'test': str(test),
            'status': 'ERROR',
            'time': elapsed,
            'module': test.__class__.__module__,
            'error': self._exc_info_to_string(err, test)
        })

    def addFailure(self, test, err):
        super().addFailure(test, err)
        elapsed = time.time() - self.test_start_time
        self.test_results.append({
            'test': str(test),
            'status': 'FAIL',
            'time': elapsed,
            'module': test.__class__.__module__,
            'error': self._exc_info_to_string(err, test)
        })

    def addSkip(self, test, reason):
        super().addSkip(test, reason)
        elapsed = time.time() - self.test_start_time
        self.test_results.append({
            'test': str(test),
            'status': 'SKIP',
            'time': elapsed,
            'module': test.__class__.__module__,
            'reason': reason
        })


class TestSuiteRunner:
    """Comprehensive test suite runner with detailed reporting."""

    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.results = None

    def discover_and_run_tests(self, test_dir='tests', pattern='test_*.py'):
        """Discover and run all tests."""
        print("=" * 80)
        print("STRATEGYBUILDER PRO - COMPREHENSIVE TEST SUITE")
        print("=" * 80)
        print(f"Test Discovery: {test_dir}")
        print(f"Pattern: {pattern}")
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        print()

        # Discover tests
        loader = unittest.TestLoader()
        suite = loader.discover(test_dir, pattern=pattern)

        # Count total tests
        total_tests = suite.countTestCases()
        print(f"ğŸ“Š Discovered {total_tests} tests")
        print()

        # Run tests
        print("ğŸ§ª Running tests...")
        print()

        self.start_time = time.time()

        # Create custom test runner
        stream = StringIO()
        runner = unittest.TextTestRunner(
            stream=stream,
            verbosity=2,
            resultclass=ColoredTextTestResult
        )

        self.results = runner.run(suite)
        self.end_time = time.time()

        return self.results

    def generate_report(self):
        """Generate comprehensive test report."""
        if not self.results:
            print("âŒ No test results available")
            return

        elapsed = self.end_time - self.start_time

        # Gather statistics
        total = self.results.testsRun
        passed = total - len(self.results.failures) - len(self.results.errors) - len(self.results.skipped)
        failed = len(self.results.failures)
        errors = len(self.results.errors)
        skipped = len(self.results.skipped)

        # Calculate pass rate
        pass_rate = (passed / total * 100) if total > 0 else 0

        # Print summary header
        print()
        print("=" * 80)
        print("TEST EXECUTION SUMMARY")
        print("=" * 80)
        print()

        # Overall statistics
        print("ğŸ“ˆ OVERALL STATISTICS:")
        print(f"   Total Tests Run:     {total}")
        print(f"   âœ… Passed:           {passed} ({pass_rate:.1f}%)")
        print(f"   âŒ Failed:           {failed}")
        print(f"   ğŸ’¥ Errors:           {errors}")
        print(f"   â­ï¸  Skipped:          {skipped}")
        print(f"   â±ï¸  Execution Time:   {elapsed:.2f}s")
        print()

        # Module breakdown
        self._print_module_breakdown()

        # Detailed failures and errors
        if failed > 0 or errors > 0:
            print()
            print("=" * 80)
            print("DETAILED FAILURES AND ERRORS")
            print("=" * 80)
            print()

            if failed > 0:
                print("âŒ FAILURES:")
                print()
                for test, traceback in self.results.failures:
                    print(f"  Test: {test}")
                    print(f"  {'-' * 76}")
                    print(f"  {traceback}")
                    print()

            if errors > 0:
                print("ğŸ’¥ ERRORS:")
                print()
                for test, traceback in self.results.errors:
                    print(f"  Test: {test}")
                    print(f"  {'-' * 76}")
                    print(f"  {traceback}")
                    print()

        # Skipped tests
        if skipped > 0:
            print()
            print("=" * 80)
            print("SKIPPED TESTS")
            print("=" * 80)
            print()
            for test, reason in self.results.skipped:
                print(f"  â­ï¸  {test}")
                print(f"     Reason: {reason}")
                print()

        # Final verdict
        print()
        print("=" * 80)
        print("FINAL VERDICT")
        print("=" * 80)
        print()

        if failed == 0 and errors == 0:
            print("  ğŸ‰ ALL TESTS PASSED!")
            print(f"  âœ¨ {passed} tests passed successfully")
            if skipped > 0:
                print(f"  â„¹ï¸  {skipped} tests skipped (optional dependencies)")
            print()
            print("  âœ… PROJECT IS READY FOR BETA RELEASE")
        else:
            print("  âš ï¸  TESTS FAILED")
            print(f"  âŒ {failed + errors} test(s) need attention")
            print()
            print("  ğŸ”§ PLEASE FIX FAILING TESTS BEFORE RELEASE")

        print()
        print("=" * 80)

        return passed, failed, errors, skipped, elapsed

    def _print_module_breakdown(self):
        """Print test results broken down by module."""
        # Organize results by module
        module_results = {}

        for result in self.results.test_results:
            module = result['module']
            if module not in module_results:
                module_results[module] = {
                    'passed': 0,
                    'failed': 0,
                    'errors': 0,
                    'skipped': 0,
                    'total': 0,
                    'time': 0
                }

            module_results[module]['total'] += 1
            module_results[module]['time'] += result['time']

            if result['status'] == 'PASS':
                module_results[module]['passed'] += 1
            elif result['status'] == 'FAIL':
                module_results[module]['failed'] += 1
            elif result['status'] == 'ERROR':
                module_results[module]['errors'] += 1
            elif result['status'] == 'SKIP':
                module_results[module]['skipped'] += 1

        print("ğŸ“¦ RESULTS BY MODULE:")
        print()

        # Sort by module name
        for module in sorted(module_results.keys()):
            stats = module_results[module]
            status_icon = "âœ…" if stats['failed'] == 0 and stats['errors'] == 0 else "âŒ"

            print(f"  {status_icon} {module}")
            print(f"     Total: {stats['total']} | "
                  f"âœ… {stats['passed']} | "
                  f"âŒ {stats['failed']} | "
                  f"ğŸ’¥ {stats['errors']} | "
                  f"â­ï¸  {stats['skipped']} | "
                  f"â±ï¸  {stats['time']:.2f}s")
            print()

    def save_report_to_file(self, filename='test_report.txt'):
        """Save report to file."""
        if not self.results:
            return

        with open(filename, 'w') as f:
            # Redirect stdout to file
            old_stdout = sys.stdout
            sys.stdout = f

            self.generate_report()

            sys.stdout = old_stdout

        print(f"ğŸ“„ Report saved to: {filename}")

    def save_json_report(self, filename='test_report.json'):
        """Save detailed report in JSON format."""
        if not self.results:
            return

        elapsed = self.end_time - self.start_time
        total = self.results.testsRun
        passed = total - len(self.results.failures) - len(self.results.errors) - len(self.results.skipped)

        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total': total,
                'passed': passed,
                'failed': len(self.results.failures),
                'errors': len(self.results.errors),
                'skipped': len(self.results.skipped),
                'execution_time': elapsed,
                'pass_rate': (passed / total * 100) if total > 0 else 0
            },
            'test_results': self.results.test_results,
            'failures': [
                {'test': str(test), 'traceback': tb}
                for test, tb in self.results.failures
            ],
            'errors': [
                {'test': str(test), 'traceback': tb}
                for test, tb in self.results.errors
            ],
            'skipped': [
                {'test': str(test), 'reason': reason}
                for test, reason in self.results.skipped
            ]
        }

        with open(filename, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"ğŸ“„ JSON report saved to: {filename}")


def check_environment():
    """Check and report on environment setup."""
    print()
    print("=" * 80)
    print("ENVIRONMENT CHECK")
    print("=" * 80)
    print()

    # Check Python version
    print(f"ğŸ Python Version: {sys.version.split()[0]}")

    # Check critical imports
    critical_modules = [
        'pandas',
        'numpy',
        'backtrader',
        'yfinance',
        'plotly',
        'streamlit',
        'yaml'
    ]

    optional_modules = [
        'sklearn',
        'xgboost',
        'reportlab',
        'weasyprint'
    ]

    print()
    print("ğŸ“¦ Critical Dependencies:")
    all_critical_ok = True
    for module in critical_modules:
        try:
            __import__(module)
            print(f"   âœ… {module}")
        except ImportError:
            print(f"   âŒ {module} - MISSING!")
            all_critical_ok = False

    print()
    print("ğŸ“¦ Optional Dependencies:")
    for module in optional_modules:
        try:
            __import__(module)
            print(f"   âœ… {module}")
        except ImportError:
            print(f"   âš ï¸  {module} - not installed (optional)")

    print()

    if not all_critical_ok:
        print("âš ï¸  WARNING: Some critical dependencies are missing!")
        print("   Install with: pip install -r requirements.txt")
        print()
        return False

    return True


def main():
    """Main test runner."""
    # Check environment first
    env_ok = check_environment()

    if not env_ok:
        print()
        print("=" * 80)
        print("âš ï¸  ENVIRONMENT CHECK FAILED")
        print("=" * 80)
        print()
        print("Please install missing dependencies before running tests.")
        print("Run: pip install -r requirements.txt")
        print()
        return 1

    # Run tests
    runner = TestSuiteRunner()
    results = runner.discover_and_run_tests()

    # Generate reports
    passed, failed, errors, skipped, elapsed = runner.generate_report()

    # Save reports to files
    print()
    print("=" * 80)
    print("SAVING REPORTS")
    print("=" * 80)
    print()

    runner.save_report_to_file('test_report.txt')
    runner.save_json_report('test_report.json')

    print()
    print("=" * 80)
    print("BETA RELEASE CHECKLIST")
    print("=" * 80)
    print()

    # Beta release checklist
    checklist = [
        ("All critical tests passing", failed == 0 and errors == 0),
        ("No import errors", True),  # Would have failed earlier
        ("Environment validated", env_ok),
        ("Test coverage > 40%", passed > 100),  # Rough estimate
        ("Documentation present", os.path.exists('README.md')),
        ("Configuration files present", os.path.exists('config/default_config.yaml'))
    ]

    all_checks_pass = True
    for item, status in checklist:
        icon = "âœ…" if status else "âŒ"
        print(f"  {icon} {item}")
        if not status:
            all_checks_pass = False

    print()

    if all_checks_pass and failed == 0 and errors == 0:
        print("  ğŸ‰ ALL CHECKS PASSED - READY FOR BETA RELEASE!")
    else:
        print("  âš ï¸  Some checks failed - address issues before release")

    print()
    print("=" * 80)

    # Return exit code
    if failed > 0 or errors > 0:
        return 1

    return 0


if __name__ == '__main__':
    sys.exit(main())
